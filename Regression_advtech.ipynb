{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshmaapps/ml/blob/main/Regression_advtech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCiLpViKTF8d"
      },
      "source": [
        "               \n",
        "The problem is to predict housing prices (houses) based on their characteristics. This is a regression problem because we try to predict a continuous value instead of a binary value.\n",
        "\n",
        "\n",
        "\n",
        "# Contents\n",
        "1. [Importing Packages](#p1)\n",
        "2. [Loading Data](#p2)\n",
        "3. [Inspecting Data: Target Variable and its correlation](#p3)\n",
        "4. [Imputing Null Values](#p4)\n",
        "5. [Feature Engineering](#p5)\n",
        "6. [ML Models](#p6)\n",
        "7. [Model Comparison](#p7)\n",
        "8. [Parameter tuning for Gradient Boosting](#p8)\n",
        "9. [Blending + Submission](#p9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wxgokteTF8e"
      },
      "source": [
        "<a id=\"p1\"></a>\n",
        "# 1.  Importing Packages\n",
        "We will need as usual the numpy and pandas libraries to work with numbers and data, seaborn and matplotlib to visualize data. We would also like to filter out unnecessary warnings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Sa1HggmTF8f",
        "outputId": "09b8e8d9-9d95-4819-9987-5bdc4e5dbf64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.7.3)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.2->seaborn) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2022.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "! pip3 install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B7t0n331TF8g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUdkDBEgTF8h"
      },
      "source": [
        "<a id=\"p2\"></a>\n",
        "# 2. Loading and Inspecting Data\n",
        "We will try to load our Training and Test data set with some Pandas functions as well as inspect it in order to get an idea of the data we're working with. It is necessary to understand the data features before to start running any model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N20mbKd4yrSX",
        "outputId": "1199ddbf-3adb-405a-e49d-48c23454d29a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "avy9jqHkTF8h"
      },
      "outputs": [],
      "source": [
        "#Data_train\n",
        "df_train = pd.read_csv('train.csv')\n",
        "#Data_test\n",
        "df_test = pd.read_csv('test.csv')\n",
        "df_train = df_train.drop([\"Last Sold On\", \"Address\", \"Id\", \"Summary\", \"Region\", \"Elementary School\", \"Middle School\", \"Elementary School Distance\",\"Middle School Distance\", \"High School\", \"High School Distance\"], axis=1)\n",
        "df_test = df_test.drop([\"Last Sold On\", \"Address\", \"Id\", \"Summary\", \"Region\", \"Elementary School\", \"Middle School\", \"Elementary School Distance\",\"Middle School Distance\", \"High School\", \"High School Distance\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')\n",
        "df_train.to(device, non_blocking=True)"
      ],
      "metadata": {
        "id": "OHlnVXJk0mng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydUvl79STF8i"
      },
      "outputs": [],
      "source": [
        "df_train.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zmg9m_bnTF8i"
      },
      "outputs": [],
      "source": [
        "df_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "aKuhm27eTF8i"
      },
      "outputs": [],
      "source": [
        "df_train.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI2vHk6-TF8i"
      },
      "source": [
        "<a id=\"p3\"></a>\n",
        "# 3. Inspecting Data: Target Variable and its Correlation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzQad1rhTF8j"
      },
      "outputs": [],
      "source": [
        "#histogram\n",
        "df_train['Sold Price'].hist(bins = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWJWRMG5TF8j"
      },
      "outputs": [],
      "source": [
        "(np.log(df_train[\"Sold Price\"])).hist(bins = 40)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_9IH7XVTF8j"
      },
      "source": [
        "We can clearly see that the target variable has a normal ditribution that is skewed towards the left. Now let's calculate the Skewness and Kurtosis :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6ip_NAwTF8k",
        "outputId": "3d44c881-c09b-4aad-c86a-02a4603be0ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skewness: 12.513866\n",
            "Kurtosis: 325.084888\n"
          ]
        }
      ],
      "source": [
        "#skewness & kurtosis\n",
        "print(\"Skewness: %f\" % df_train['Sold Price'].skew())\n",
        "print(\"Kurtosis: %f\" % df_train['Sold Price'].kurt())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKib9BwBTF8k"
      },
      "source": [
        "As we've seen before, there are so many columns to work with, so let's try to figure out the correlations to get a better idea of which columns are strongly related to the Sale Price of the house. This will help us eliminating the features that won't do a good job predicting the Sale Price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "sy05LZmLTF8k"
      },
      "outputs": [],
      "source": [
        "#correlation matrix\n",
        "corrmat = df_train.corr()\n",
        "#Plot a heatmap to visualize the correlations\n",
        "f, ax = plt.subplots(figsize=(30, 19))\n",
        "sns.set(font_scale=1.45)\n",
        "sns.heatmap(corrmat, square=True,cmap='coolwarm');\n",
        "correlations = corrmat[\"Sold Price\"].sort_values(ascending=False)\n",
        "features = correlations.index[0:15]\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZsysKE8TF8l"
      },
      "outputs": [],
      "source": [
        "corrmat = df_train.corr()\n",
        "\n",
        "correlations = corrmat[\"Price\"].sort_values(ascending=False)\n",
        "features = correlations.index[0:15]\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "kjoVbjr1TF8l"
      },
      "outputs": [],
      "source": [
        "#sns.pairplot(df_train[features], size = 2.5)\n",
        "#plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw_Gl0XkTF8l"
      },
      "source": [
        "<a id=\"p4\"></a>\n",
        "# 4. Imputing Null Values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRc4TtDMTF8m"
      },
      "source": [
        "First of all, we will start with dropping the Id column , as it doesn't add any information for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vBlcfqxTF8m"
      },
      "source": [
        "In this large data, we have a lot of missing values in the cells. In order to effectively train our model, we must first deal with the missing values. There are missing values for both numerical and categorical data.\n",
        "\n",
        "For numerical imputing, we will try to fill the missing values with the mean. For categorical imputing, I chose to fill the missing values with the most common term that appeared from the entire column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmKDbIHjTF8m"
      },
      "source": [
        "## NaN values are important\n",
        "In fact, the NaN values actually mean something in some columns. This means that if a value is NaN, the house might not have that certain attribute, which will affect the price of the house.\n",
        "So, we will try to fill in the null cell with a new category called \"None\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ncrH5FhATF8m"
      },
      "outputs": [],
      "source": [
        "#Replacing every Nan value with \"None\"\n",
        "null_with_meaning = [ \"Heating\", \"Cooling\", \"Heating features\", \"Cooling features\", \"Garage spaces\",\"Parking\",\"Parking features\", \"Appliances included\", \"Laundry features\"]\n",
        "for i in null_with_meaning:\n",
        "    df_train[i].fillna(\"None\", inplace=True)\n",
        "    df_test[i].fillna(\"None\", inplace=True)\n",
        "\n",
        "# missing Full bathrooms, Total interior livable area, Total spaces, Elementary School Score,  \n",
        "#Middle School Score, High School Score, Flooring,\n",
        "meanVals = [ \"Lot\", \"Year built\", \"Total interior livable area\", \"Total spaces\", \"Elementary School Score\", \"Middle School Score\", \"High School Score\" ]\n",
        "for i in meanVals:\n",
        "    df_train[i].fillna(df_train[i].mean(), inplace=True)\n",
        "    df_test[i].fillna(df_test[i].mean(), inplace=True)\n",
        "    \n",
        "\n",
        "df_train[\"Flooring\"].fillna(\"Carpet\", inplace=True)\n",
        "df_test[\"Flooring\"].fillna(\"Carpet\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "j8n7MW-STF8n"
      },
      "outputs": [],
      "source": [
        "beds = df_train[\"Bedrooms\"]\n",
        "testbeds = df_test[\"Bedrooms\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baths = df_train[\"Bathrooms\"]\n",
        "testbaths = df_test[\"Bathrooms\"]"
      ],
      "metadata": {
        "id": "_1ItMW_1GcBG"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "nm_I-H1yTF8o"
      },
      "outputs": [],
      "source": [
        "def fixParam(x):\n",
        "    index = 0\n",
        "    for i in x:\n",
        "      #if len(i) >2:\n",
        "       # print(i)\n",
        "        #x.at[index] = '3.0'\n",
        "      if str(i) == 'nan':\n",
        "        x.at[index] = '2.0'\n",
        "      index = index + 1\n",
        "    return x\n",
        "\n",
        "baths = fixParam(baths)\n",
        "testbaths = fixParam(testbaths)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(baths.nunique())\n",
        "baths = baths.astype(float)\n",
        "df_train[\"Bathrooms\"] = baths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SaSP9-LGrQa",
        "outputId": "635bc2a6-0be2-422c-c5dd-c85c6b5a077f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(beds.nunique())\n",
        "print(testbeds.nunique())\n",
        "beds = beds.astype(float)\n",
        "df_train[\"Bedrooms\"] = beds\n",
        "testbeds = testbeds.astype(float)\n",
        "df_test[\"Bedrooms\"] = testbeds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lkhqo_CeE3UR",
        "outputId": "a49d876d-1a88-48ef-a452-c59d4b0191e9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31\n",
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(testbaths.nunique())\n",
        "testbaths = testbaths.astype(float)\n",
        "df_test[\"Full bathrooms\"] = testbaths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nwyiHGrBde9",
        "outputId": "71465363-9859-4dd9-f59a-7ab5bef37012"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"Full bathrooms\"] = baths\n"
      ],
      "metadata": {
        "id": "I8h20MDQCo94"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.drop(['Full bthrooms'], axis=1)"
      ],
      "metadata": {
        "id": "O2H-c81eEZ9W"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vAlraoiElWi",
        "outputId": "f62fd516-ff87-4dd9-b444-dbc4a36115e7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Sold Price', 'Type', 'Year built', 'Heating', 'Cooling', 'Parking',\n",
              "       'Lot', 'Bedrooms', 'Bathrooms', 'Full bathrooms',\n",
              "       'Total interior livable area', 'Total spaces', 'Garage spaces',\n",
              "       'Elementary School Score', 'Middle School Score', 'High School Score',\n",
              "       'Flooring', 'Heating features', 'Cooling features',\n",
              "       'Appliances included', 'Laundry features', 'Parking features',\n",
              "       'Tax assessed value', 'Annual tax amount', 'Listed On', 'Listed Price',\n",
              "       'Last Sold Price', 'City', 'Zip', 'State'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "setToValue = [\"Bedrooms\",\"Full bathrooms\", \"Bathrooms\"]\n",
        "#setToValue = [\"Bathrooms\"]\n",
        "for i in setToValue:\n",
        "    df_train = fixParam(i, df_train)\n",
        "    df_test = fixParam(i, df_test)"
      ],
      "metadata": {
        "id": "80zoNxQ2BCwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9TLyHA9TF8o",
        "outputId": "81084576-8517-4904-cc92-77d855eaea68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sold Price                         0\n",
            "Type                               0\n",
            "Year built                         0\n",
            "Heating                            0\n",
            "Cooling                            0\n",
            "Parking                            0\n",
            "Lot                                0\n",
            "Bedrooms                           0\n",
            "Bathrooms                          0\n",
            "Full bathrooms                     0\n",
            "Total interior livable area        0\n",
            "Total spaces                       0\n",
            "Garage spaces                      0\n",
            "Elementary School Score            0\n",
            "Middle School Score                0\n",
            "High School Score                  0\n",
            "Flooring                           0\n",
            "Heating features                   0\n",
            "Cooling features                   0\n",
            "Appliances included                0\n",
            "Laundry features                   0\n",
            "Parking features                   0\n",
            "Tax assessed value              3652\n",
            "Annual tax amount               4310\n",
            "Listed On                          0\n",
            "Listed Price                       0\n",
            "Last Sold Price                17766\n",
            "City                               0\n",
            "Zip                                0\n",
            "State                              0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df_train.describe()\n",
        "training_null = pd.isnull(df_train).sum()\n",
        "testing_null = pd.isnull(df_test).sum()\n",
        "\n",
        "print(training_null)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "WGJ2UmNPTF8o"
      },
      "outputs": [],
      "source": [
        "def lookup(basedOn1, basedOn2, x):\n",
        "    y = x.groupby(basedOn1)[basedOn2].apply(lambda x: x.replace(np.nan, x.value_counts().mean()))\n",
        "    #print(x[basedOn1], y) #= x = x.sort_values('col1')\n",
        "    x[basedOn2] = y\n",
        "    return x\n",
        "lookupVals = [\"Tax assessed value\", \"Annual tax amount\", \"Last Sold Price\"]\n",
        "for i in lookupVals:\n",
        "    df_train = lookup(\"Zip\", i,  df_train)\n",
        "    df_test = lookup(\"Zip\", i, df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WlXVGrhTF8p",
        "outputId": "36246eb2-6bd4-452f-8c0a-f7ab78b79b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sold Price                       0\n",
            "Type                             0\n",
            "Year built                       0\n",
            "Heating                          0\n",
            "Cooling                          0\n",
            "Parking                          0\n",
            "Lot                              0\n",
            "Bedrooms                         0\n",
            "Bathrooms                        0\n",
            "Full bathrooms                   0\n",
            "Total interior livable area      0\n",
            "Total spaces                     0\n",
            "Garage spaces                    0\n",
            "Elementary School Score          0\n",
            "Middle School Score              0\n",
            "High School Score                0\n",
            "Flooring                         0\n",
            "Heating features                 0\n",
            "Cooling features                 0\n",
            "Appliances included              0\n",
            "Laundry features                 0\n",
            "Parking features                 0\n",
            "Tax assessed value              43\n",
            "Annual tax amount               71\n",
            "Listed On                        0\n",
            "Listed Price                     0\n",
            "Last Sold Price                167\n",
            "City                             0\n",
            "Zip                              0\n",
            "State                            0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df_train.describe()\n",
        "training_null = pd.isnull(df_train).sum()\n",
        "testing_null = pd.isnull(df_test).sum()\n",
        "\n",
        "print(training_null)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGizby6UTF8p"
      },
      "outputs": [],
      "source": [
        "print(pd.isnull(df_train[\"Tax assessed value\"])==1, df_train.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "oamU1P33TF8p"
      },
      "outputs": [],
      "source": [
        "#for i in range(len(df_train[\"Tax assessed value\"])):\n",
        " #   if pd.isnull(df_train[\"Tax assessed value\"][i]):\n",
        "       # print(i, df_train[\"Zip\"][i])\n",
        "\n",
        "#for i in range(len(df_train[\"Zip\"])):\n",
        " #   if (df_train[\"Zip\"][i] == 95595):\n",
        "  #      print(i, df_train[\"Tax assessed value\"][i])\n",
        "        \n",
        "lookupVals = [\"Tax assessed value\", \"Annual tax amount\", \"Last Sold Price\"]\n",
        "for i in lookupVals:\n",
        "    df_train[i].fillna(df_train[i].mean(), inplace=True)\n",
        "    df_test[i].fillna(df_test[i].mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.to_pickle(\"unencodedTrainingSet.pkl\")\n",
        "df_test.to_pickle(\"unencodedTestSet.pkl\")"
      ],
      "metadata": {
        "id": "FF97tVuoHjxF"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFY5N-JJHvFB",
        "outputId": "7eb37888-e35c-4322-d6cd-c5aa020c823c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Sold Price', 'Type', 'Year built', 'Heating', 'Cooling', 'Parking',\n",
              "       'Lot', 'Bedrooms', 'Bathrooms', 'Full bathrooms',\n",
              "       'Total interior livable area', 'Total spaces', 'Garage spaces',\n",
              "       'Elementary School Score', 'Middle School Score', 'High School Score',\n",
              "       'Flooring', 'Heating features', 'Cooling features',\n",
              "       'Appliances included', 'Laundry features', 'Parking features',\n",
              "       'Tax assessed value', 'Annual tax amount', 'Listed On', 'Listed Price',\n",
              "       'Last Sold Price', 'City', 'Zip', 'State'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLbk216qHxyO",
        "outputId": "7590e627-aebd-46ce-a903-d549e0d85ea8"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Type', 'Year built', 'Heating', 'Cooling', 'Parking', 'Lot',\n",
              "       'Bedrooms', 'Bathrooms', 'Full bathrooms',\n",
              "       'Total interior livable area', 'Total spaces', 'Garage spaces',\n",
              "       'Elementary School Score', 'Middle School Score', 'High School Score',\n",
              "       'Flooring', 'Heating features', 'Cooling features',\n",
              "       'Appliances included', 'Laundry features', 'Parking features',\n",
              "       'Tax assessed value', 'Annual tax amount', 'Listed On', 'Listed Price',\n",
              "       'Last Sold Price', 'City', 'Zip', 'State'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTdsDfGoH0u7",
        "outputId": "a3ee495a-c4d2-49ea-abec-32cc0cb8d755"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47439, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK-i6SWWH3qT",
        "outputId": "dc93ade7-24e8-4950-a75c-70d6b356c1ab"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31626, 29)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_pickle(\"my_data.pkl\")"
      ],
      "metadata": {
        "id": "hLX9Xr-iJuQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nQLwQB_TF8p"
      },
      "source": [
        "Now, the features with a lot of missing values have been taken care of! Let's move on to the features with fewer missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "cInDaWvUTF8p",
        "outputId": "c6c271fa-eef0-4bd7-b0df-365aa20359ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical columns that will be one-hot encoded: ['Garage spaces', 'State']\n",
            "\n",
            "Categorical columns that will be dropped from the dataset: ['Type', 'Parking', 'Cooling', 'Cooling features', 'Parking features', 'Laundry features', 'Flooring', 'Heating', 'Listed On', 'Appliances included', 'Heating features', 'City']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-6470454fbfbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mOH_X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-6470454fbfbc>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Apply one-hot encoder to each column with categorical data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mOH_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Series' object is not callable"
          ]
        }
      ],
      "source": [
        "def encode(x):\n",
        "    # \"Cardinality\" means the number of unique values in a column\n",
        "    # Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
        "    object_cols = [col for col in x.columns if x[col].dtype == \"object\"]\n",
        "\n",
        "    low_cardinality_cols = [cname for cname in x.columns if x[cname].nunique() < 100 and \n",
        "                            x[cname].dtype == \"object\"]\n",
        "    # Columns that will be dropped from the dataset\n",
        "    high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
        "\n",
        "    print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\n",
        "    print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)\n",
        "\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    print (x.dtypes())\n",
        "    # Apply one-hot encoder to each column with categorical data\n",
        "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "    OH_cols = pd.DataFrame(OH_encoder.fit_transform(x[low_cardinality_cols]))\n",
        "\n",
        "    # One-hot encoding removed index; put it back\n",
        "    OH_cols.index = x.index\n",
        "\n",
        "    # Remove categorical columns (will replace with one-hot encoding)\n",
        "    num_X = x.drop(object_cols, axis=1)\n",
        "\n",
        "    # Add one-hot encoded columns to numerical features\n",
        "    OH_X = pd.concat([num_X, OH_cols], axis=1)\n",
        "    return OH_X\n",
        "\n",
        "df_train = encode(df_train)\n",
        "df_test = encode(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkUNnyRHTF8p",
        "outputId": "73966420-c230-45c4-94a6-4a608764e794"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([                 'Year built',                         'Lot',\n",
              "                         'Bathrooms',              'Full bathrooms',\n",
              "       'Total interior livable area',                'Total spaces',\n",
              "           'Elementary School Score',         'Middle School Score',\n",
              "                 'High School Score',          'Tax assessed value',\n",
              "                 'Annual tax amount',                'Listed Price',\n",
              "                   'Last Sold Price',                         'Zip',\n",
              "                                   0,                             1],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "df_test.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df_train[\"Heating\"]:\n",
        "  if i == \"None\":\n",
        "    print (i)\n",
        "    df_train[\"Heating\"] = \"0.0\"\n",
        "  if i == \"In Garage\":\n",
        "    df_train[\"Heating\"] = \"1.0\"\n",
        "  if i == \"In Garage\":\n",
        "    "
      ],
      "metadata": {
        "id": "CoGjAfRjIbnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2_OmG1VuTF8q"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.drop([0, 1 ], axis = 1)\n",
        "df_test = df_test.drop([0, 1 ] , axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "edpLZozWTF8q",
        "outputId": "a794f3de-2304-4505-8e27-a47fcf7aac62"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "(np.log(df_train[\"Sold Price\"])).hist(bins = 40)\n",
        "#It appears that adding the logarithm has made the target SalePrice more normally distributed. Machine Learning models tend to work much better with normally distributed targets, rather than greatly skewed targets. By transforming the prices, we can improve the performance later.\n",
        "y_train = np.log(df_train[\"Sold Price\"])\n",
        "df_train = df_train.drop(['Sold Price'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um4YjUbFTF8q"
      },
      "source": [
        "<a id=\"p6\"></a>\n",
        "# 6. Process : ML Models\n",
        "Now that we've explored the data, we can begin to build and test different models for regression to predict the SalePrice of each house.\n",
        "In classification, we used accuracy as a evaluation metric. In regression, we will use the R^2 score as well as the RMSE to evaluate our model performance. We will also use cross validation to optimize our model hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_Tm1HjTNTF8q"
      },
      "outputs": [],
      "source": [
        "#Importing all the librairies we'll need\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score, KFold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUZ3EyCwTF8q"
      },
      "source": [
        "### Defining Training/Test Sets\n",
        "\n",
        "We have already dropped the Id column for the training set since those are not involved in predicting the Sale Price of a house. We will also drop The SalePrice column from our training dataset and make LogPrice our target instead. This will improve model performance and yield a much smaller RMSE because of the scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVzSNw0yTF8q"
      },
      "source": [
        "### Splitting into Validation\n",
        "\n",
        "Try to split our training data again into validation sets. This will help us evaluate our model performance and maybe avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8I6wuQ7yTF8q"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split #to create validation data set\n",
        "\n",
        "X_training, X_valid, y_training, y_valid = train_test_split(df_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96JqNGTXTF8q"
      },
      "source": [
        "## Linear Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "yhuh215RTF8r"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lm = LinearRegression()\n",
        "lm.fit(X_training,y_training)\n",
        "print(lm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcW6rxpjTF8r"
      },
      "source": [
        "**Model Evaluation**\n",
        "Let's evaluate the model by checking out it's coefficients and how we can interpret them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxWUJtuATF8r"
      },
      "outputs": [],
      "source": [
        "# print the intercept\n",
        "print(lm.intercept_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VDM22tJkTF8r"
      },
      "outputs": [],
      "source": [
        "print(lm.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW8Ip9mRTF8r"
      },
      "source": [
        "**Predictions from our Model** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNCNWE9ZTF8r"
      },
      "outputs": [],
      "source": [
        "predictions = lm.predict(X_valid)\n",
        "predictions= predictions.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "xD-V-0XITF8r"
      },
      "outputs": [],
      "source": [
        "submission_predictions = np.exp(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "jJFI_YazTF8r"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "print('MAE:', metrics.mean_absolute_error(y_valid, submission_predictions))\n",
        "print('MSE:', metrics.mean_squared_error(y_valid, submission_predictions))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, submission_predictions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2COBKLZeTF8s"
      },
      "source": [
        "**Adding the GridSearchCV function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1u8zTMpTF8s"
      },
      "outputs": [],
      "source": [
        "linreg = LinearRegression()\n",
        "parameters_lin = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"copy_X\" : [True, False]}\n",
        "grid_linreg = GridSearchCV(linreg, parameters_lin, verbose=1 , scoring = \"r2\")\n",
        "grid_linreg.fit(X_training, y_training)\n",
        "\n",
        "print(\"Best LinReg Model: \" + str(grid_linreg.best_estimator_))\n",
        "print(\"Best Score: \" + str(grid_linreg.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "k4JO9ogRTF8s"
      },
      "outputs": [],
      "source": [
        "linreg = grid_linreg.best_estimator_\n",
        "linreg.fit(X_training, y_training)\n",
        "lin_pred = linreg.predict(X_valid)\n",
        "r2_lin = r2_score(y_valid, lin_pred)\n",
        "rmse_lin = np.sqrt(mean_squared_error(y_valid, lin_pred))\n",
        "print(\"R^2 Score: \" + str(r2_lin))\n",
        "print(\"RMSE Score: \" + str(rmse_lin))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfKoJpFZTF8s"
      },
      "outputs": [],
      "source": [
        "scores_lin = cross_val_score(linreg, X_training, y_training, cv=10, scoring=\"r2\")\n",
        "print(\"Cross Validation Score: \" + str(np.mean(scores_lin)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMCgvCgMTF8s"
      },
      "source": [
        "## Ridge Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38KqpoLFTF8s"
      },
      "outputs": [],
      "source": [
        "ridge = Ridge()\n",
        "parameters_ridge = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"copy_X\" : [True, False], \"solver\" : [\"auto\"]}\n",
        "grid_ridge = GridSearchCV(ridge, parameters_ridge, verbose=1, scoring=\"r2\")\n",
        "grid_ridge.fit(X_training, y_training)\n",
        "\n",
        "print(\"Best Ridge Model: \" + str(grid_ridge.best_estimator_))\n",
        "print(\"Best Score: \" + str(grid_ridge.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVs92w-xTF8s"
      },
      "outputs": [],
      "source": [
        "ridge = grid_ridge.best_estimator_\n",
        "ridge.fit(X_training, y_training)\n",
        "ridge_pred = ridge.predict(X_valid)\n",
        "r2_ridge = r2_score(y_valid, ridge_pred)\n",
        "rmse_ridge = np.sqrt(mean_squared_error(y_valid, ridge_pred))\n",
        "print(\"R^2 Score: \" + str(r2_ridge))\n",
        "print(\"RMSE Score: \" + str(rmse_ridge))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ25lIMWTF8s"
      },
      "outputs": [],
      "source": [
        "scores_ridge = cross_val_score(ridge, X_training, y_training, cv=10, scoring=\"r2\")\n",
        "print(\"Cross Validation Score: \" + str(np.mean(scores_ridge)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAn8-sGDTF8s"
      },
      "source": [
        "## Gradient Boosting Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6E91xrwpTF8t"
      },
      "outputs": [],
      "source": [
        "from sklearn import ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpMyDLIITF8t",
        "outputId": "be406a37-fd8b-43b3-afd5-fcc16ced7587"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(learning_rate=0.05, loss='ls', max_depth=5,\n",
              "                          n_estimators=200)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "params = {'n_estimators': 200, 'max_depth': 5, 'min_samples_split': 2,\n",
        "          'learning_rate': 0.05, 'loss': 'ls' }\n",
        "clf = ensemble.GradientBoostingRegressor(**params)\n",
        "\n",
        "clf.fit(X_training, y_training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDwWeIz8TF8t",
        "outputId": "d06dfd4e-f6a0-44e3-9fed-3521058b3003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 Score: 0.9368220402156536\n",
            "RMSE Score: 0.19962770369880276\n"
          ]
        }
      ],
      "source": [
        "clf_pred=clf.predict(X_valid)\n",
        "clf_pred= clf_pred.reshape(-1,1)\n",
        "r2_clf = r2_score(y_valid, clf_pred)\n",
        "rmse_clf = np.sqrt(mean_squared_error(y_valid, clf_pred))\n",
        "print(\"R^2 Score: \" + str(r2_clf))\n",
        "print(\"RMSE Score: \" + str(rmse_clf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfvKFUsPTF8t",
        "outputId": "405d608c-f5a2-4860-9e46-8c18d528cba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Validation Score: 0.9333775275436238\n"
          ]
        }
      ],
      "source": [
        "scores_clf = cross_val_score(clf, X_training, y_training, cv=10, scoring=\"r2\")\n",
        "print(\"Cross Validation Score: \" + str(np.mean(scores_clf)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2-jc2keTF8t"
      },
      "source": [
        "## Decision Tree Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPKKQQPPTF8t"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "dtreg = DecisionTreeRegressor(random_state = 100)\n",
        "parameters_dtr = {\"criterion\" : [\"mse\", \"friedman_mse\", \"mae\"], \"splitter\" : [\"best\", \"random\"], \"min_samples_split\" : [2, 3, 5, 10], \n",
        "                  \"max_features\" : [\"auto\", \"log2\"]}\n",
        "grid_dtr = GridSearchCV(dtreg, parameters_dtr, verbose=1, scoring=\"r2\")\n",
        "grid_dtr.fit(X_training, y_training)\n",
        "\n",
        "print(\"Best DecisionTreeRegressor Model: \" + str(grid_dtr.best_estimator_))\n",
        "print(\"Best Score: \" + str(grid_dtr.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njaIt9zPTF8t"
      },
      "outputs": [],
      "source": [
        "dtr = grid_dtr.best_estimator_\n",
        "dtreg.fit(X_training, y_training)\n",
        "dtr_pred = dtreg.predict(X_valid)\n",
        "r2_dtr = r2_score(y_valid, dtr_pred)\n",
        "rmse_dtr = np.sqrt(mean_squared_error(y_valid, dtr_pred))\n",
        "print(\"R^2 Score: \" + str(r2_dtr))\n",
        "print(\"RMSE Score: \" + str(rmse_dtr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5v7MWPPTF8t"
      },
      "outputs": [],
      "source": [
        "scores_dtr = cross_val_score(dtreg, X_training, y_training, cv=10, scoring=\"r2\")\n",
        "print(\"Cross Validation Score: \" + str(np.mean(scores_dtr)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl1XrvNITF8u"
      },
      "source": [
        "## Random Forest Regression "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K_J9716TF8u",
        "outputId": "8b7fb428-0f3d-4391-93fc-8062abcde473"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
          ]
        }
      ],
      "source": [
        "rfr = RandomForestRegressor()\n",
        "paremeters_rf = {\"n_estimators\" : [5, 10, 15, 20], \"criterion\" : [\"mse\" , \"mae\"], \"min_samples_split\" : [2, 3, 5, 10], \n",
        "                 \"max_features\" : [\"auto\", \"log2\"]}\n",
        "grid_rf = GridSearchCV(rfr, paremeters_rf, verbose=1, scoring=\"r2\")\n",
        "grid_rf.fit(X_training, y_training)\n",
        "\n",
        "print(\"Best RandomForestRegressor Model: \" + str(grid_rf.best_estimator_))\n",
        "print(\"Best Score: \" + str(grid_rf.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxaVygDtTF8u"
      },
      "outputs": [],
      "source": [
        "rf = grid_rf.best_estimator_\n",
        "rfr.fit(X_training, y_training)\n",
        "rf_pred = rfr.predict(X_valid)\n",
        "r2_rf = r2_score(y_valid, rf_pred)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_valid, rf_pred))\n",
        "print(\"R^2 Score: \" + str(r2_rf))\n",
        "print(\"RMSE Score: \" + str(rmse_rf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "C4A-gptxTF8u"
      },
      "outputs": [],
      "source": [
        "scores_rf = cross_val_score(rfr, X_training, y_training, cv=10, scoring=\"r2\")\n",
        "print(\"Cross Validation Score: \" + str(np.mean(scores_rf)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiwJ8ankTF8u",
        "outputId": "c75c4804-c26b-41ef-da5c-03fc8c2bbc17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting xgboost\n",
            "  Using cached xgboost-1.6.2-py3-none-macosx_12_0_arm64.whl (1.5 MB)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from xgboost) (1.22.3)\n",
            "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from xgboost) (1.8.0)\n",
            "Installing collected packages: xgboost\n",
            "Successfully installed xgboost-1.6.2\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "!{sys.executable} -m pip install xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlIqKb8KTF8v"
      },
      "source": [
        "## Xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm-j_JE7TF8v",
        "outputId": "2c575412-3351-4831-de92-772f7399b612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 Score: 0.9376785368879113\n",
            "RMSE Score: 0.19826992072359856\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "xgboost = XGBRegressor(learning_rate=0.01,n_estimators=20000,\n",
        "                                     max_depth=3, min_child_weight=0,\n",
        "                                     gamma=0, subsample=0.7,\n",
        "                                     colsample_bytree=0.7,\n",
        "                                     objective='reg:squarederror', nthread=-1,\n",
        "                                     scale_pos_weight=1, seed=27,\n",
        "                                     reg_alpha=0.006)\n",
        "xgb = xgboost.fit(X_training, y_training)\n",
        "xgb_pred = xgb.predict(X_valid)\n",
        "r2_xgb = r2_score(y_valid, xgb_pred)\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_valid, xgb_pred))\n",
        "print(\"R^2 Score: \" + str(r2_xgb))\n",
        "print(\"RMSE Score: \" + str(rmse_xgb))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgboost.save('myHousingAdvModelxgb')\n",
        "clf.save('myHousingAdvModelgb')\n",
        "rfr.save('myHousingAdvModelRf')"
      ],
      "metadata": {
        "id": "LgJi1wBfYBsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lrl1gxscTF8v"
      },
      "outputs": [],
      "source": [
        "!pip install lightgbm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylKoClvBTF8v"
      },
      "source": [
        "### LGBM Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "zD-fivzcTF8v"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "lightgbm = LGBMRegressor(objective='regression', \n",
        "                                       num_leaves=4,\n",
        "                                       learning_rate=0.01, \n",
        "                                       n_estimators=20000,\n",
        "                                       max_bin=2000, \n",
        "                                       bagging_fraction=0.75,\n",
        "                                       bagging_freq=5, \n",
        "                                       bagging_seed=7,\n",
        "                                       feature_fraction=0.2,\n",
        "                                       feature_fraction_seed=7,\n",
        "                                       verbose=-1,\n",
        "                                       )\n",
        "gbm = lightgbm.fit(X_training, y_training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-GsvMRVTF8v"
      },
      "outputs": [],
      "source": [
        "gbm_pred = gbm.predict(X_valid)\n",
        "r2_gbm = r2_score(y_valid, gbm_pred)\n",
        "rmse_gbm = np.sqrt(mean_squared_error(y_valid, gbm_pred))\n",
        "print(\"R^2 Score: \" + str(r2_gbm))\n",
        "print(\"RMSE Score: \" + str(rmse_gbm))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRhn5isgTF8v"
      },
      "outputs": [],
      "source": [
        "rmse_gbm = 0.0\n",
        "r2_gbm = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8H1jcvoTF8v"
      },
      "source": [
        "<a id=\"p7\"></a>\n",
        "# 7. Model Comparison\n",
        "After applying different models and evaluating them, now we will use test data to predict the LogPrice with the most adequat one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GshPl7WXTF8v"
      },
      "outputs": [],
      "source": [
        "model_performances = pd.DataFrame({\n",
        "    \"Model\" : [\"Linear Regression\", \"Ridge\", \"Decision Tree Regressor\", \"Random Forest Regressor\",\"Gradient Boosting Regression\",\"XGBoost\",\"LGBM Regressor\"],\n",
        "    \"R Squared\" : [str(r2_lin)[0:5], str(r2_ridge)[0:5],  str(r2_dtr)[0:5], str(r2_rf)[0:5] , str(r2_clf)[0:5], str(r2_xgb)[0:5], str(r2_gbm)[0:5]],\n",
        "    \"RMSE\" : [str(rmse_lin)[0:8], str(rmse_ridge)[0:8],  str(rmse_dtr)[0:8], str(rmse_rf)[0:8], str(rmse_clf)[0:8], str(rmse_xgb)[0:8], str(rmse_gbm)[0:8]]\n",
        "})\n",
        "model_performances.round(4)\n",
        "\n",
        "print(\"Sorted by R Squared:\")\n",
        "model_performances.sort_values(by=\"R Squared\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "nz3JtjlwTF8w"
      },
      "outputs": [],
      "source": [
        "print(\"Sorted by RMSE:\")\n",
        "model_performances.sort_values(by=\"RMSE\", ascending=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwj38W1wTF8w"
      },
      "source": [
        "Finally, I decided to use the XGBoost on the test set because I believe it will perform the best based on the comparison above. It has a high R^2 value and a low RMSE.\n",
        "But before doing that, let's try to improve our Gradient Boosting Regression model by tuning its parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eZ4fLPQTF8w"
      },
      "source": [
        "<a id=\"p8\"></a>\n",
        "# 8. Parameter tuning for Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOjImTC-TF8w"
      },
      "source": [
        "## Learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnX3pgpjTF8w"
      },
      "outputs": [],
      "source": [
        "learning_rates = [0.75 ,0.5, 0.25, 0.1, 0.05, 0.01]\n",
        "\n",
        "r2_results = []\n",
        "rmse_results = []\n",
        "\n",
        "for eta in learning_rates:\n",
        "    model = ensemble.GradientBoostingRegressor(learning_rate=eta)\n",
        "    model.fit(X_training, y_training)\n",
        "    y_pred = model.predict(X_valid)\n",
        "    r2_clf = r2_score(y_valid, y_pred)\n",
        "    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
        "    r2_results.append(r2_clf)\n",
        "    rmse_results.append(rmse_clf)\n",
        "    \n",
        "from matplotlib.legend_handler import HandlerLine2D\n",
        "line1, = plt.plot(learning_rates, r2_results, 'b', label='R^2')\n",
        "line2, = plt.plot(learning_rates, rmse_results, 'r', label='RMSE')\n",
        "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('learning_rates')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zfWfDjfTF8w"
      },
      "source": [
        "We see that using a high learning rate results in overfitting. For this data, a learning rate of 0.05 is optimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doEpeV_ETF8w"
      },
      "source": [
        "## N_estimators\n",
        "N_estimators represents the number of trees in the forest. Usually the higher the number of trees the better to learn the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "595cPnExTF8w"
      },
      "outputs": [],
      "source": [
        "n_estimators = [1, 2, 16, 32, 64, 100, 200, 500]\n",
        "r2_results = []\n",
        "rmse_results = []\n",
        "\n",
        "for estimator in n_estimators:\n",
        "    model = ensemble.GradientBoostingRegressor(n_estimators=estimator)\n",
        "    model.fit(X_training, y_training)\n",
        "    y_pred = model.predict(X_valid)\n",
        "    r2_clf = r2_score(y_valid, y_pred)\n",
        "    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
        "    r2_results.append(r2_clf)\n",
        "    rmse_results.append(rmse_clf)\n",
        "    \n",
        "from matplotlib.legend_handler import HandlerLine2D\n",
        "line1, = plt.plot(n_estimators, r2_results, 'b', label='R^2')\n",
        "line2, = plt.plot(n_estimators, rmse_results, 'r', label='RMSE')\n",
        "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('n_estimators')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDqgAKx6TF8w"
      },
      "source": [
        "In our case, using more than 100 trees is good for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FamRExLaTF8w"
      },
      "source": [
        "## Max_depth\n",
        "This indicates how deep the built tree can be. The deeper the tree, the more splits it has and it captures more information about how the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU4AUdOuTF8w"
      },
      "outputs": [],
      "source": [
        "max_depths = np.linspace(1, 10, 10, endpoint=True)\n",
        "r2_results = []\n",
        "rmse_results = []\n",
        "\n",
        "for max_depth in max_depths:\n",
        "    max_depth = int(max_depth)\n",
        "    model = ensemble.GradientBoostingRegressor(max_depth=max_depth)\n",
        "    model.fit(X_training, y_training)\n",
        "    y_pred = model.predict(X_valid)\n",
        "    r2_clf = r2_score(y_valid, y_pred)\n",
        "    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
        "    r2_results.append(r2_clf)\n",
        "    rmse_results.append(rmse_clf)\n",
        "    \n",
        "from matplotlib.legend_handler import HandlerLine2D\n",
        "line1, = plt.plot(max_depths, r2_results, 'b', label='R^2')\n",
        "line2, = plt.plot(max_depths, rmse_results, 'r', label='RMSE')\n",
        "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('max_depths')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq9Wia5QTF8w"
      },
      "source": [
        "Based on the plots above, we will choose a max_depth = 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPiguWcwTF8x"
      },
      "source": [
        "## Min_samples_split\n",
        "min_samples_split represents the minimum number of samples required to split an internal node. This can vary between considering at least one sample at each node to considering all of the samples at each node. When we increase this parameter, the tree becomes more constrained as it has to consider more samples at each node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RceOYBRTF8x"
      },
      "outputs": [],
      "source": [
        "min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
        "r2_results = []\n",
        "rmse_results = []\n",
        "\n",
        "for min_samples_split in min_samples_splits:\n",
        "    model = ensemble.GradientBoostingRegressor(min_samples_split=min_samples_split)\n",
        "    model.fit(X_training, y_training)\n",
        "    y_pred = model.predict(X_valid)\n",
        "    r2_clf = r2_score(y_valid, y_pred)\n",
        "    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
        "    r2_results.append(r2_clf)\n",
        "    rmse_results.append(rmse_clf)\n",
        "    \n",
        "from matplotlib.legend_handler import HandlerLine2D\n",
        "line1, = plt.plot(min_samples_splits, r2_results, 'b', label='R^2')\n",
        "line2, = plt.plot(min_samples_splits, rmse_results, 'r', label='RMSE')\n",
        "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('min_samples_splits')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-sNX3sgTF8x"
      },
      "source": [
        "So we will choose min_samples_splits = 0.28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-bRYjFjTF8x"
      },
      "source": [
        "## Min_samples_leaf\n",
        "The minimum number of samples required to be at a leaf node. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryZY50yQTF8x"
      },
      "outputs": [],
      "source": [
        "min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
        "r2_results = []\n",
        "rmse_results = []\n",
        "\n",
        "for min_samples_leaf in min_samples_leafs:\n",
        "    model = ensemble.GradientBoostingRegressor(min_samples_leaf=min_samples_leaf)\n",
        "    model.fit(X_training, y_training)\n",
        "    y_pred = model.predict(X_valid)\n",
        "    r2_clf = r2_score(y_valid, y_pred)\n",
        "    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
        "    r2_results.append(r2_clf)\n",
        "    rmse_results.append(rmse_clf)\n",
        "    \n",
        "from matplotlib.legend_handler import HandlerLine2D\n",
        "line1, = plt.plot(min_samples_leafs, r2_results, 'b', label='R^2')\n",
        "line2, = plt.plot(min_samples_leafs, rmse_results, 'r', label='RMSE')\n",
        "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('min_samples_leafs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykv73t49TF8x"
      },
      "source": [
        " Increasing this value can cause underfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rlHEa9dTF8x"
      },
      "source": [
        "## Max_features\n",
        "max_features represents the number of features to consider when looking for the best split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u10hp107TF8x"
      },
      "outputs": [],
      "source": [
        "max_features = list(range(1,30,1))\n",
        "r2_results = []\n",
        "rmse_results = []\n",
        "\n",
        "for max_feature in max_features:\n",
        "    model = ensemble.GradientBoostingRegressor(max_features=max_feature)\n",
        "    model.fit(X_training, y_training)\n",
        "    y_pred = model.predict(X_valid)\n",
        "    r2_clf = r2_score(y_valid, y_pred)\n",
        "    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
        "    r2_results.append(r2_clf)\n",
        "    rmse_results.append(rmse_clf)\n",
        "    \n",
        "from matplotlib.legend_handler import HandlerLine2D\n",
        "line1, = plt.plot(max_features, r2_results, 'b', label='R^2')\n",
        "line2, = plt.plot(max_features, rmse_results, 'r', label='RMSE')\n",
        "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('max_features')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fiAt1lDTF8x"
      },
      "outputs": [],
      "source": [
        "#Updating model with tuned hyperparams\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "xgboost = XGBRegressor(                                     gamma=0, subsample=0.7,\n",
        "                                     max_depth = 3, n_estimators =20000,learning_rate = 0.15,\n",
        "                                     colsample_bytree=0.7,\n",
        "                                     objective='reg:squarederror', nthread=-1,\n",
        "                                     scale_pos_weight=1, seed=27,\n",
        "                                     reg_alpha=0.006)\n",
        "xgb = xgboost.fit(X_training, y_training)\n",
        "xgb_pred = xgb.predict(X_valid)\n",
        "r2_xgb = r2_score(y_valid, xgb_pred)\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_valid, xgb_pred))\n",
        "print(\"R^2 Score: \" + str(r2_xgb))\n",
        "print(\"RMSE Score: \" + str(rmse_xgb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIBTJ1yaTF8x"
      },
      "outputs": [],
      "source": [
        "X_training.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmaezXV6TF8x"
      },
      "source": [
        "<a id=\"p9\"></a>\n",
        "# 9. Blending + Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erJsgHajTF8x"
      },
      "outputs": [],
      "source": [
        "def blend_models_predict(X):\n",
        "    return ((0.05 * lm.predict(X)) + \\\n",
        "            (0.05 * linreg.predict(X)) + \\\n",
        "            (0.05 * ridge.predict(X)) + \\\n",
        "            (0.1 * clf.predict(X)) + \\\n",
        " #           (0.2 * gbm.predict(X)) + \\\n",
        "            (0.15 * rfr.predict(X)) + \\\n",
        "            (0.4 * xgb.predict(X)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMI6ZIoeTF8y"
      },
      "outputs": [],
      "source": [
        "submission_predictions = np.exp(blend_models_predict(df_test))\n",
        "print(submission_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDTUtot_TF8y"
      },
      "outputs": [],
      "source": [
        "res=pd.DataFrame(columns = ['Id', 'SalePrice'])\n",
        "res['Id'] = df_test.index + 1461\n",
        "res['SalePrice'] = submission_predictions\n",
        "res.to_csv('submission1.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuIHCCM5TF8y"
      },
      "source": [
        "## Best Score : 0.11777\n",
        "\n",
        "## Leaderboard Rank : 775"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}